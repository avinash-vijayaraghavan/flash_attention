{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c980287-6e95-49a1-bff3-98006399e410",
   "metadata": {},
   "source": [
    "#### Build the cuda attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4b101b-8dbb-4e31-8bfd-5d22d7d70c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running install\n",
      "/home/avinash/anaconda3/lib/python3.12/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "/home/avinash/anaconda3/lib/python3.12/site-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` and ``easy_install``.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating custom_cuda.egg-info\n",
      "writing custom_cuda.egg-info/PKG-INFO\n",
      "writing dependency_links to custom_cuda.egg-info/dependency_links.txt\n",
      "writing top-level names to custom_cuda.egg-info/top_level.txt\n",
      "writing manifest file 'custom_cuda.egg-info/SOURCES.txt'\n",
      "/home/avinash/anaconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  warnings.warn(msg.format('we could not find ninja.'))\n",
      "reading manifest file 'custom_cuda.egg-info/SOURCES.txt'\n",
      "writing manifest file 'custom_cuda.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_ext\n",
      "/home/avinash/anaconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.6) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
      "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
      "/home/avinash/anaconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no g++ version bounds defined for CUDA version 12.6\n",
      "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
      "building 'cuda_attention' extension\n",
      "creating build/temp.linux-x86_64-cpython-312\n",
      "/home/avinash/anaconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "/usr/local/cuda-12.6/bin/nvcc -I/home/avinash/anaconda3/lib/python3.12/site-packages/torch/include -I/home/avinash/anaconda3/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/home/avinash/anaconda3/lib/python3.12/site-packages/torch/include/TH -I/home/avinash/anaconda3/lib/python3.12/site-packages/torch/include/THC -I/usr/local/cuda-12.6/include -I/home/avinash/anaconda3/include/python3.12 -c attention.cu -o build/temp.linux-x86_64-cpython-312/attention.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=cuda_attention -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 -std=c++17\n",
      "\u001b[01m\u001b[0m\u001b[01mattention.cu(961)\u001b[0m: \u001b[01;35mwarning\u001b[0m #940-D: missing return statement at end of non-void function \u001b[01m\"attention_forward1\"\u001b[0m\n",
      "  }\n",
      "  ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01mattention.cu(367)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"b\"\u001b[0m was declared but never referenced\n",
      "      int b = idx / (NH * T);\n",
      "          ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01mattention.cu(368)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"nh\"\u001b[0m was declared but never referenced\n",
      "      int nh = idx % (NH * T);\n",
      "          ^\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01mattention.cu(961)\u001b[0m: \u001b[01;35mwarning\u001b[0m #940-D: missing return statement at end of non-void function \u001b[01m\"attention_forward1\"\u001b[0m\n",
      "  }\n",
      "  ^\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01mattention.cu(932)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"col_tiles\"\u001b[0m was declared but never referenced\n",
      "      int col_tiles = row_tiles;\n",
      "          ^\n",
      "\n",
      "g++ -pthread -B /home/avinash/anaconda3/compiler_compat -fno-strict-overflow -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/avinash/anaconda3/include -fPIC -O2 -isystem /home/avinash/anaconda3/include -fPIC -I/home/avinash/anaconda3/lib/python3.12/site-packages/torch/include -I/home/avinash/anaconda3/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/home/avinash/anaconda3/lib/python3.12/site-packages/torch/include/TH -I/home/avinash/anaconda3/lib/python3.12/site-packages/torch/include/THC -I/usr/local/cuda-12.6/include -I/home/avinash/anaconda3/include/python3.12 -c attention_binding.cpp -o build/temp.linux-x86_64-cpython-312/attention_binding.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=cuda_attention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "creating build/lib.linux-x86_64-cpython-312\n",
      "g++ -pthread -B /home/avinash/anaconda3/compiler_compat -fno-strict-overflow -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/avinash/anaconda3/include -fPIC -O2 -isystem /home/avinash/anaconda3/include -pthread -B /home/avinash/anaconda3/compiler_compat -shared build/temp.linux-x86_64-cpython-312/attention.o build/temp.linux-x86_64-cpython-312/attention_binding.o -L/home/avinash/anaconda3/lib/python3.12/site-packages/torch/lib -L/usr/local/cuda-12.6/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-312/cuda_attention.cpython-312-x86_64-linux-gnu.so\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "copying build/lib.linux-x86_64-cpython-312/cuda_attention.cpython-312-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
      "creating stub loader for cuda_attention.cpython-312-x86_64-linux-gnu.so\n",
      "byte-compiling build/bdist.linux-x86_64/egg/cuda_attention.py to cuda_attention.cpython-312.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying custom_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying custom_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying custom_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying custom_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "__pycache__.cuda_attention.cpython-312: module references __file__\n",
      "creating dist\n",
      "creating 'dist/custom_cuda-0.0.0-py3.12-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing custom_cuda-0.0.0-py3.12-linux-x86_64.egg\n",
      "removing '/home/avinash/anaconda3/lib/python3.12/site-packages/custom_cuda-0.0.0-py3.12-linux-x86_64.egg' (and everything under it)\n",
      "creating /home/avinash/anaconda3/lib/python3.12/site-packages/custom_cuda-0.0.0-py3.12-linux-x86_64.egg\n",
      "Extracting custom_cuda-0.0.0-py3.12-linux-x86_64.egg to /home/avinash/anaconda3/lib/python3.12/site-packages\n",
      "Adding custom-cuda 0.0.0 to easy-install.pth file\n",
      "\n",
      "Installed /home/avinash/anaconda3/lib/python3.12/site-packages/custom_cuda-0.0.0-py3.12-linux-x86_64.egg\n",
      "Processing dependencies for custom-cuda==0.0.0\n",
      "Finished processing dependencies for custom-cuda==0.0.0\n"
     ]
    }
   ],
   "source": [
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3180b8-bd6f-480b-aeaa-ab29e783a6bf",
   "metadata": {},
   "source": [
    "#### import\n",
    "    - flash_attention.py contains the triton version and its equivalent torch version\n",
    "    - looking at the torch version we can follow the code in the triton version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec044375-447f-40b5-8496-e5594f2bd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, cuda_attention, flash_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c169c-a00a-43db-9db0-d4e89ff12e49",
   "metadata": {},
   "source": [
    "#### samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a1d810-4ff4-42ae-8b5e-353724d1b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, num_heads, head_dim = (\n",
    "            4,\n",
    "            4096,\n",
    "            32,\n",
    "            32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08256c7b-9bbb-40d4-9b31-2585efc146f0",
   "metadata": {},
   "source": [
    "#### query, key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab0ae97-1b86-4fc6-8f74-59fd3128b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "q = torch.randn(batch_size,num_heads, seq_len, head_dim, device='cuda') # query\n",
    "k, v = q,q\n",
    "kp = q.transpose(2,3).contiguous()  # the triton version uses shape (batch_size,num_heads, head_dim, seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a468649-0329-4b01-8151-b9b7e2265f94",
   "metadata": {},
   "source": [
    "#### torch version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ff6813-5277-4e36-9469-0e90754e85e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref_out: torch.Size([4, 32, 4096, 32])\n"
     ]
    }
   ],
   "source": [
    "ref_out = torch.nn.functional.scaled_dot_product_attention(q,k,v, is_causal=True, scale=None)\n",
    "print(f\"ref_out: {ref_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dee66e-d210-4fd2-a304-afee169792c1",
   "metadata": {},
   "source": [
    "#### custom flash attention \n",
    "    -- flash_attention\n",
    "        -- using loops (for understanding the triton version)\n",
    "    -- its slow, comment out for larger shapes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e1ec2a-6ebf-458c-98a8-c7a3ada669fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT (SLOW)\n",
    "\n",
    "# torch_out = flash_attention.flash_attention(q,k,v)\n",
    "# print(f\"torch_out: {ref_out.shape}\")\n",
    "# print(torch.allclose(ref_out, torch_out, atol=1e-3, rtol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c11a3-2764-48ed-9090-41ac76a1a53d",
   "metadata": {},
   "source": [
    "#### triton version\n",
    "    -- triton_flash_attention which uses the kernel: flash_attention_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff72f94d-dc28-4f9c-9fdf-07d45632f896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid: (256, 128, 1)\n",
      "triton_out: torch.Size([4, 32, 4096, 32])\n"
     ]
    }
   ],
   "source": [
    "triton_out = flash_attention.triton_flash_attention(q, kp,v)\n",
    "print(f\"triton_out: {triton_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e9f9d-f642-4b30-874f-ab5f687234e5",
   "metadata": {},
   "source": [
    "#### cuda version \n",
    "    -- kernels are found in attention.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51af0e7-007e-45c4-a07b-cf8d49859cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_out: torch.Size([4, 32, 4096, 32])\n"
     ]
    }
   ],
   "source": [
    "cuda_out = cuda_attention.attention_forward(q,k,v, True, True)\n",
    "print(f\"cuda_out: {cuda_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e09a31-1bb0-4316-88c4-01ca739fc57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# in some its not matching. need to dig further\n",
    "\n",
    "if not torch.allclose(triton_out, ref_out,atol=1e-2, rtol=1e-3):\n",
    "    print(f\"triton_out, ref_out not matching. Try with larger values of atol and rtol\\n\")\n",
    "    print(f\"Sample:\\nref_out: {ref_out[-1,-1,-1,-5:]}\\ntriton_out: {triton_out[-1,-1,-1,-5:]}\")\n",
    "else:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4265a38a-9fe6-4717-bf4c-e91c8daa8782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if not torch.allclose(cuda_out, ref_out,atol=1e-2, rtol=1e-3):\n",
    "    print(f\"cuda_out, ref_out not matching. Try with larger values of atol and rtol\\n\")\n",
    "    print(f\"cuda_out: {cuda_out[-1,-1,-1,-5:]}\\nref_out: {ref_out[-1,-1,-1,-5:]}\\ntriton_out: {triton_out[-1,-1,-1,-5:]}\")\n",
    "else:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4ccf0-85c2-417d-91f2-e19e20f13258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
